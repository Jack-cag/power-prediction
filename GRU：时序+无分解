#######################调用第三方库#############################
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout,GRU
from keras import optimizers 
from keras.optimizers import Adam
seed = 1234
np.random.seed(seed)
plt.style.use('ggplot')

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
#%%#######################绘制出一列数据的信息####################################
# In[1]                     加载数据风电数据
Area1_Load = pd.read_csv('Area1_Load.csv',index_col='YMD', parse_dates=['YMD'])
Area2_Load = pd.read_csv('Area2_Load.csv',index_col='YMD', parse_dates=['YMD'])
Area1_Weather = pd.read_csv('Area1_Weather.csv',index_col='YMD', parse_dates=['YMD'])
Area2_Weather = pd.read_csv('Area1_Weather.csv',index_col='YMD', parse_dates=['YMD'])

# In[2]                    将提取出来的数据整理格式
startday=60
length=200
name='Area1_Load'
Area_Load=Area1_Load
Area_Weather=Area1_Weather
Area_Load.describe()



dataset = Area_Load.iloc[startday:startday+length, :].values
dataset = np.concatenate(dataset).T
dataset = pd.DataFrame(dataset)
time_index = pd.date_range(start=Area_Load.iloc[startday].name, periods=96*length, freq='15min')  # 生成一年时间范围内每隔15分钟的时间索引
dataset.set_index(time_index, inplace=True)
dataset.rename(columns={0: name}, inplace=True)
# In[3]                   绘制制定负荷数据并进行归一化处理
print(' Count row of data: ',len(dataset))
fig = plt.figure(figsize=(14, 6))
plt.plot(dataset)
plt.xlabel('Time')
plt.ylabel(name)
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.title(name)
plt.show()
#Min-Max Normalization
dataset_norm = dataset.copy()
dataset[[name]]
scaler = MinMaxScaler()
dataset_norm[name] = scaler.fit_transform(dataset[[name]])
dataset_norm

fig = plt.figure(figsize=(14, 6))
plt.plot(dataset_norm)
plt.xlabel('Time')
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.title(name+' Normalized')
plt.show()
# In[4]                     划分训练集，测试集，验证集并绘制  
# Partition data into data train, val & test
totaldata = dataset.values
totaldatatrain = int(len(totaldata)*0.7)#训练个数
totaldataval = int(len(totaldata)*0.1)#验证个数
totaldatatest = int(len(totaldata)*0.2)#测试个数

# Store data into each partition
training_set = dataset_norm[0:totaldatatrain]
val_set=dataset_norm[totaldatatrain:totaldatatrain+totaldataval]
test_set = dataset_norm[totaldatatrain+totaldataval:]


# graph of data training
fig = plt.figure(figsize=(14, 6))
plt.plot(training_set)
plt.xlabel(name)
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.title(name+' Training')
plt.show()


# graph of data validation
fig = plt.figure(figsize=(14, 6))
plt.plot(val_set)
plt.xlabel(name)
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.title(name+' Validation')
val_set


# graph of data test
fig = plt.figure(figsize=(14, 6))
plt.plot(test_set)
plt.xlabel(name)
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))
plt.title(name+' Test')
plt.show()
test_set
# In[5]                         实现滑动窗口
# Initiaton value of lag
lag = 2
# sliding windows function
def create_sliding_windows(data,len_data,lag):
    x=[]
    y=[]
    for i in range(lag,len_data):
        x.append(data[i-lag:i,0])
        y.append(data[i,0]) 
    return np.array(x),np.array(y)

# Formating data into array for create sliding windows
array_training_set = np.array(training_set)
array_val_set = np.array(val_set)
array_test_set = np.array(test_set)

# Create sliding windows into training data
x_train, y_train = create_sliding_windows(array_training_set,len(array_training_set), lag)
x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))
# Create sliding windows into validation data
x_val,y_val = create_sliding_windows(array_val_set,len(array_val_set),lag)
x_val = np.reshape(x_val, (x_val.shape[0],x_val.shape[1],1))
# Create sliding windows into test data
x_test,y_test = create_sliding_windows(array_test_set,len(array_test_set),lag)
x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))

# In[6]
# Hyperparameters
learning_rate = 0.0001
hidden_unit = 64
batch_size=256
epoch = 100

# Architecture Gated Recurrent Unit
regressorGRU = Sequential()

# First GRU layer with dropout
regressorGRU.add(GRU(units=hidden_unit, return_sequences=True, input_shape=(x_train.shape[1],1), activation = 'tanh'))
regressorGRU.add(Dropout(0.2))
# Second GRU layer with dropout
regressorGRU.add(GRU(units=hidden_unit, return_sequences=True, activation = 'tanh'))
regressorGRU.add(Dropout(0.2))
# Third GRU layer with dropout
regressorGRU.add(GRU(units=hidden_unit, return_sequences=False, activation = 'tanh'))
regressorGRU.add(Dropout(0.2))

# Output layer
regressorGRU.add(Dense(units=1))

# Compiling the Gated Recurrent Unit
regressorGRU.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error')

# Fitting ke data training dan data validation
pred = regressorGRU.fit(x_train, y_train, validation_data=(x_val,y_val), batch_size=batch_size, epochs=epoch)

# In[7]
# Graph model loss (train loss & val loss)
fig = plt.figure(figsize=(10, 4))
plt.plot(pred.history['loss'], label='train loss')
plt.plot(pred.history['val_loss'], label='val loss')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(loc='upper right')
plt.show()

# In[8]
# Tabel value of training loss & validation loss
learningrate_parameter = learning_rate
train_loss=pred.history['loss'][-1]
validation_loss=pred.history['val_loss'][-1]
learningrate_parameter=pd.DataFrame(data=[[learningrate_parameter, train_loss, validation_loss]],
                                    columns=['Learning Rate', 'Training Loss', 'Validation Loss'])
learningrate_parameter.set_index('Learning Rate')

# In[9]
# Implementation model into data test
y_pred_test = regressorGRU.predict(x_test)

# Invert normalization min-max
y_pred_invert_norm = scaler.inverse_transform(y_pred_test)

# In[10]
# Comparison data test with data prediction
datacompare = pd.DataFrame()
datatest=np.array(dataset[name][totaldatatrain+totaldataval+lag:])
datapred= y_pred_invert_norm

datacompare['Data Test'] = datatest
datacompare['Prediction Results'] = datapred
datacompare

# In[11]

# Calculatre value of Root Mean Square Error 
def rmse(datatest, datapred):
    return np.round(np.sqrt(np.mean((datapred - datatest) ** 2)), 4)
print('Result Root Mean Square Error Prediction Model :',rmse(datatest, datapred))

def mape(datatest, datapred): 
    return np.round(np.mean(np.abs((datatest - datapred) / datatest) * 100), 4)
    
print('Result Mean Absolute Percentage Error Prediction Model : ', mape(datatest, datapred), '%')

# In[12]
# Create graph data test and prediction result
plt.figure(num=None, figsize=(10, 4), dpi=80,facecolor='w', edgecolor='k')
plt.title('Graph Comparison Data Actual and Data Prediction')
plt.plot(datacompare['Data Test'], color='red',label='Data Test')
plt.plot(datacompare['Prediction Results'], color='blue',label='Prediction Results')
plt.xlabel('Day')
plt.ylabel(name)
plt.legend()
plt.show()

